---
description: >-
  Papers that fall outside general reasoning frameworks/architecture that caught
  my eye.
---

# Interesting Papers/Resources

**What's the Magic Word? A Control Theory of LLM Prompting**\
[https://arxiv.org/pdf/2310.04444](https://arxiv.org/pdf/2310.04444) &#x20;

SELF-DISCOVER and implementation\
[https://arxiv.org/pdf/2402.03620](https://arxiv.org/pdf/2402.03620)

{% embed url="https://github.com/catid/self-discover" %}

**DSPY**

{% embed url="https://github.com/stanfordnlp/dspy" %}

**The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits**

[**https://arxiv.org/pdf/2402.17764**](https://arxiv.org/pdf/2402.17764)



**Towards Understanding Grokking: An Effective Theory of Representation Learning**

[**https://proceedings.neurips.cc/paper\_files/paper/2022/file/dfc310e81992d2e4cedc09ac47eff13e-Paper-Conference.pdf**](https://proceedings.neurips.cc/paper\_files/paper/2022/file/dfc310e81992d2e4cedc09ac47eff13e-Paper-Conference.pdf)
